**Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms**



### abstract

与现有关于多智能体强化学习（MARL）的综述正交，我们强调了MARL理论的几个新角度和分类，包括在扩展型博弈中的学习、具有网络化智能体的去中心化MARL、均值场条件下的MARL、用于博弈学习的基于策略方法的（非）收敛性等。本章的总体目标不仅是提供对多智能体强化学习领域当前状况的评估，还在于确定在MARL理论研究方面有潜力的未来研究方向。我们希望本章能继续激发对这一令人兴奋且充满挑战的话题感兴趣的研究人员的研究热情。

### 1 Introduction

大体上，根据所处理的环境类型，多智能体强化学习（MARL）算法可以分为三类：完全合作型、完全竞争型以及两者混合型。特别是，在合作环境中，代理会合作以优化共同的长期回报；而在竞争环境中，代理的回报通常总和为零。

有几个存在于MARL领域的共同的挑战：

1. MARL 中的学习目标是多维的，因为所有智能体的目标不一定一致，这带来了处理均衡点的挑战，以及一些超越回报优化的额外性能标准，例如通信/协调的效率，以及对潜在对抗智能体的鲁棒性。
2. 由于所有智能体都在根据自身利益同时改进其策略，每个智能体所面临的环境变得非平稳。这打破或使单智能体环境中大多数理论分析的基本框架失效。
3. 随着智能体数量呈指数增长的联合动作空间可能会导致可扩展性问题，这被称为多智能体强化学习的组合性质
4. 在多智能体强化学习中，信息结构，即谁知道什么，更为复杂，因为每个智能体只能有限地获取其他智能体的观测，从而可能导致局部决策规则不理想。



### 2 Background

#### 2.2 Multi-Agent RL Framework

##### 2.2.1 Markov/Stochastic Games

<img src="img/image-20260126150641959.png" alt="image-20260126150641959" style="zoom:75%;" />

经典的马尔可夫博弈（Markov Game / Stochastic Game）通常假设是“全状态可观测”的；也就是说，在模型定义层面，每个 agent 都可以观测到同一个全局状态。而且在这种统一的数学模型下，agents之间的关系可以是合作（cooperative） / 对抗（competitive） / 混合（mixed, general-sum）。

##### 2.2.2 Extensive Form Game

![image-20260126170614157](img/image-20260126170614157.png)

![image-20260126172422586](img/image-20260126172422586.png)

### 3 Challenges in MARL Theory

#### 3.1 Non-Unique Learning Goals

**在多智能体强化学习中，“学习目标”本身往往不是唯一的，这与单智能体强化学习有本质区别。**

在单智能体强化学习中，学习目标通常是明确且唯一的，例如最大化期望累积回报，对应一个最优策略或最优价值函数。然而，在多智能体场景下，问题本质上是一个博弈。不同智能体的目标相互耦合，一个智能体的最优行为依赖于其他智能体的策略选择，因此系统中**通常不存在唯一的“最优解”**。

具体而言，多智能体系统往往存在**多个均衡解**（如多个 Nash 均衡），而这些均衡在性能、公平性或稳定性上可能彼此不同。即便所有智能体的奖励函数已经给定，学习过程仍可能收敛到不同的策略组合，而且这些结果在理论上都是合理的解。

因此，该小节强调：**多智能体强化学习面临的首要挑战之一，并不是“如何更快收敛”，而是“应该收敛到什么”**。在缺乏额外约束或协调机制的情况下，学习算法并不能保证选择某一个特定的均衡，也无法保证不同训练过程会得到一致的结果。

该小节的结论是：**由于学习目标的非唯一性，多智能体强化学习的评估和比较不能简单照搬单智能体的最优性标准，而需要明确所采用的解概念以及对均衡选择的偏好。**

#### 3.2 Non-Stationarity

在单智能体强化学习中，环境转移概率和奖励函数是固定不变的，因此可以将经验样本视为来自同一个分布，并依赖这一假设证明诸如 Q-learning 等算法的收敛性。然而，在多智能体场景下，其他智能体也在同时更新各自的策略，这意味着环境动力学会随着时间发生变化。对任一智能体而言，其观察到的状态转移和奖励分布会不断漂移，从而违反了马尔可夫性和平稳性假设。

因此，即便环境在全局层面可以被建模为一个 Markov game，只要采用的是**独立学习（independent learning）范式**，单个智能体所面对的问题就不再等价于一个标准的 MDP。这会导致经验回放失效、价值函数估计偏差累积，并使经典强化学习算法可能无法收敛，或者只能收敛到次优解。

**多智能体强化学习中的主要困难并非来自随机性本身，而是来自其他智能体策略随时间变化所引入的非平稳性**。这也是后续需要引入集中式训练、对手建模、博弈论均衡概念或信息结构假设的根本动机。

#### 3.3 Scalability Issue

在多智能体系统中，环境状态往往需要同时刻画多个智能体的状态，而动作空间通常是所有智能体动作的笛卡尔积。随着智能体数量的增加，联合状态空间和联合动作空间会呈指数级增长，这使得基于全局状态或联合动作建模的方法在存储、计算和采样效率上都变得不可行。

该小节进一步指出，即便采用集中式训练或全局 critic 来缓解非平稳性问题，也往往需要访问完整的全局状态和联合动作信息，这在大规模系统中会带来严重的通信、计算和样本复杂度负担。因此，这类方法在理论上可行，但在大规模多智能体任务中难以直接应用。

**可扩展性是多智能体强化学习的基本瓶颈之一，迫使研究者在性能最优性与计算可行性之间做权衡，并推动了参数共享、局部信息建模、分解式价值函数以及去中心化执行等方法的发展。**

#### 3.4 Various Information Structures

在许多理论分析中，往往假设所有智能体都可以观测到完整的全局状态或其他智能体的行为，但在实际应用中，这种假设通常并不成立。智能体往往只能获得局部、噪声或延迟的观测信息，甚至无法直接感知其他智能体的存在。

作者进一步区分了多种常见的信息结构，包括完全可观测、部分可观测、集中式信息与去中心化信息等情形，并指出这些差异会直接影响策略表示、学习算法设计以及可达到的性能上限。例如，在去中心化和部分可观测的条件下，单个智能体很难准确推断系统整体状态，从而使协调行为的学习变得更加困难。

多智能体强化学习中信息结构具有高度多样性，**不同的可观测性和信息共享假设会根本性地影响问题难度和算法设计**。

多智能体强化学习（MARL）中的三种典型信息结构

（a）集中式设置 (Centralized setting) 在这种结构下，系统存在一个中央控制器。该控制器可以汇总所有智能体的联合动作、联合奖励和联合观测值，并据此为所有智能体统一设计策略。智能体与控制器之间的交互主要表现为：智能体向控制器发送私有观测数据，控制器则回传为每个智能体设计的本地策略。

（b）具有网络连接的分布式设置 (Decentralized setting with networked agents) 这种结构没有中央控制器，智能体之间通过一个通信网络连接。智能体只能与自己的直接邻居进行信息交换，而本地信息正是通过这种邻居间的通信在整个网络中逐步传播的。这种设置在协作式多智能体强化学习（Cooperative MARL）中非常普遍。

（c）完全分布式设置 (Fully decentralized setting) 这是最极端的去中心化情况。系统中既没有中央控制器，智能体之间也不存在任何显式的信息交换。每个智能体完全仅凭自己的本地观测值来做出决策并更新策略。这种结构通常对应于“独立学习”（Independent Learning）方案，即每个智能体都将其他智能体视为环境的一部分。