**Multi‑agent deep reinforcement learning: a survey**



### 2  Background



定义2 马尔可夫博弈：马尔可夫博弈是马尔可夫决策过程在多智能体场景下的扩展，它由多个交互智能体、所有智能体共同观察的状态集合、联合动作空间、状态转移概率函数、每个智能体各自的奖励函数以及折扣因子组成。在马尔可夫博弈中，多个智能体同时在共享环境中交互，每个智能体根据自身策略选择动作，环境根据联合动作进行状态转移，每个智能体收到与自身相关的即时奖励信号。

定义3 最佳响应：当智能体i面对其他智能体的固定策略时，如果存在一个策略使得该智能体在所有状态下都能获得不低于其他任何策略的价值函数，那么这个策略就是对该联合策略的最佳响应。简单来说，最佳响应是智能体在其他智能体策略不变的情况下能获得最大长期收益的策略。

定义4 纳什均衡：纳什均衡是一种策略组合，其中每个智能体的策略都是对其他智能体策略的最佳响应。在这种均衡状态下，没有任何一个智能体能够通过单方面改变自己的策略来提高自己的收益。直观理解就是，当所有其他智能体的策略固定时，每个智能体都没有动机偏离当前策略。

定义5 帕累托最优：当一个联合策略在所有状态下对所有智能体的价值都不低于另一个联合策略，并且至少存在一个智能体在某个状态下获得严格更高的价值。如果一个纳什均衡没有被其他任何策略帕累托支配，即不存在能让至少一个智能体获益而其他智能体不受损的替代策略，那么这个纳什均衡就是帕累托最优的。帕累托最优性是用来比较多个智能体联合策略优劣的一个概念。当一个联合策略帕累托占优另一个联合策略时，意味着在所有状态下，每个智能体从这个策略中获得的价值都不低于另一个策略所能提供的价值，并且至少存在一个智能体在某些状态下能获得严格更高的价值。换句话说，第一个策略让所有智能体至少过得和第二个策略一样好，同时至少有一个智能体过得更好，而没有任何智能体因此受损。一个纳什均衡如果达到了帕累托最优，就表示不存在其他策略能让所有智能体的整体收益得到改进——任何试图让某些智能体获益的策略调整，都必然会导致至少一个其他智能体的收益下降。这种状态代表了多智能体系统中一种相对高效的合作结果。

定义6 非平稳性：在多智能体环境中，由于所有智能体同时学习并调整自己的策略，从单个智能体的视角来看，环境的动态特性会不断变化。当其他智能体的策略发生改变时，相同状态和动作组合可能导致不同的状态转移结果，这就使得环境对单个学习者呈现出非平稳特性，也称为"移动目标问题"。

定义7 阴影均衡：当存在一个联合策略，使得至少有一个智能体单方面偏离该策略所能获得的改进，不大于偏离另一个联合策略所能获得的改进时，前者就被后者所阴影。阴影均衡描述了一种病理现象，即智能体可能收敛到次优的均衡点，因为该均衡被另一个更优的均衡所"阴影"，导致智能体难以发现更优解。阴影均衡描述的是多智能体学习中一种容易陷入次优解的现象。当存在两个联合策略时，如果其中一个策略（称为被阴影覆盖的策略）使得至少有一个智能体在单独偏离该策略时，所能获得的收益提升非常有限，甚至不如从另一个策略偏离时能获得的最小收益，那么这个策略就被认为是被"阴影覆盖"的。简单来说，就是某个看似合理的均衡点实际上被另一个更优的均衡点"遮挡"了，导致学习算法难以发现真正最优的解决方案。这种现象常出现在独立学习的智能体中，因为每个智能体在训练过程中只能观察到局部信息，无法准确判断自己的行为对整体结果的贡献。当环境存在随机性或者奖励信号不稳定时，智能体可能过早地收敛到一个次优的纳什均衡，而无法继续探索通向更优解的路径。相对过度泛化是阴影均衡的一种具体表现，指的是智能体倾向于选择那些在面对其他智能体各种可能行为时都表现尚可的策略，而不是专门针对最优协作方案进行优化，最终导致团队整体性能低于理论上的最优水平。

定义8 部分可观察马尔可夫博弈：部分可观察马尔可夫博弈是马尔可夫博弈的扩展，其中智能体无法直接观察到环境的完整全局状态，而只能获取部分观测信息。每个智能体拥有自己的观测空间，该观测空间是全局状态空间的子集。在这种设定下，智能体必须基于有限的局部观测和历史交互信息来做出决策，增加了学习的复杂性。

定义9 信用分配问题：在完全合作的多智能体场景中，所有智能体共享相同的团队奖励信号，但单个智能体难以判断自己在联合行动中对最终团队奖励的具体贡献。由于无法将团队获得的奖励准确归因于各个智能体的个体行动，智能体可能无法获得有效的学习信号，这种困境被称为信用分配问题。该问题在部分可观察环境中尤为突出。

### 3  Analysis of training schemes

多智能体深度强化学习中的训练方案主要分为两大类：分布式训练和集中式训练。这两种范式的核心区别在于智能体在学习过程中是否共享信息。

分布式训练指的是每个智能体完全独立地学习，不与其他智能体交换任何信息。每个智能体只能根据自己观察到的局部环境和获得的奖励来更新自己的策略。这种训练方式简单直接，符合现实世界中许多自主系统的设定，但存在明显缺陷：由于其他智能体的策略也在同时变化，从单个智能体的视角看，环境变得不稳定，就像在移动的目标上射击一样困难。这种非平稳性使得学习过程变得低效，尤其在复杂或随机性较强的环境中表现更差。早期研究发现，独立学习的智能体在简单确定性环境中可能表现尚可，但在更复杂的场景中往往难以找到好的合作策略。

集中式训练则允许智能体在学习阶段共享额外信息，比如其他智能体的观察结果、采取的动作或策略参数。这种信息共享只在训练期间使用，测试或实际部署时可能会被移除，以保证系统最终能够分布式运行。集中式训练又分为两种执行方式。第一种是集中式执行，即由一个中央控制器直接为所有智能体计算联合动作。这种方式虽然简单，但随着智能体数量增加，状态和动作空间会呈指数级增长，面临"维度灾难"问题。第二种更为实用的是集中式训练配合分布式执行，智能体在训练时能访问全局信息以稳定学习过程，但实际运行时仅依靠自己的局部观察做决策。这种方法已成为当前多智能体学习的主流实践，因为它既利用了全局信息加速训练，又保留了分布式执行的可扩展性和实用性。

在集中式训练分布式执行框架下，研究者发展出多种具体技术。同构智能体可以共享神经网络参数，大幅提高学习效率；也可以通过分解联合价值函数，将复杂问题拆解为更易处理的子问题。异构智能体则常采用集中式评论家结构，即每个智能体拥有自己的策略网络（执行器），但在训练时共享一个能访问全局信息的评论家网络来评估动作质量。此外还有主从架构，由一个中央控制器协调多个分布式执行器的决策。

总的来说，训练方案的选择需要在学习效率、可扩展性和实际部署可行性之间取得平衡。集中式训练分布式执行因其兼顾训练稳定性和实际应用需求，成为当前解决多智能体协调问题最有效的方法之一。

### 4  Emergent patterns of agent behavior

当多个智能体在共享环境中同时学习和互动时，它们的行为会相互影响并产生一些意想不到的模式。这种行为模式的出现主要受到三个因素的影响：奖励结构的设计、智能体之间发展出的通信方式，以及所处的社会环境背景。

**奖励结构**是塑造智能体行为最直接的因素。当所有智能体共享相同的团队奖励时，它们倾向于学习合作策略，比如在乒乓球游戏中互相配合避免失误，或者通过惩罚整个团队来促使个体避免犯错。相反，当奖励偏向个体表现时，智能体会发展出竞争性行为，例如在物理模拟环境中学习奔跑、阻挡或擒抱对手等复杂动作。当外部奖励稀疏或缺失时，研究者会给智能体配备内在奖励机制，鼓励它们主动探索新颖状态或减少对环境动态的预测误差，从而在没有明确指导的情况下自发产生复杂行为，比如追逐、躲藏或工具使用。

**语言和通信能力**的涌现是另一个引人注目的现象。在指代类游戏中，一个智能体需要通过消息向另一个智能体描述某个目标，经过训练后它们能发展出具有一定语义结构的通信协议，甚至学会组合多个符号形成有意义的表达。当参与对话的智能体数量增加或词汇大小受到限制时，它们会自发形成更紧凑、无歧义的词汇集，并发展出一致的语法结构。在谈判类任务中，智能体甚至学会了使用欺骗等高级策略来获取更大利益。值得注意的是，虽然通信通常能提高任务表现，但数值指标并不能完全反映通信质量，因此研究者开始关注如何设计归纳偏置来促进更具解释性的语言产生。

**社会背景对行为模式的影响**体现在各种社会困境中。在公共资源困境里，智能体需要在短期个人利益和长期集体利益之间权衡，例如在共享资源有限的环境中决定是否过度开采。在公共物品困境中，智能体面临是否贡献资源以维持公共资源可持续性的选择。研究发现，智能体的行为会受到多种心理变量影响，包括对个人收益的追求、对未来后果的恐惧、对其他智能体行为的预期、相互之间的信任程度，甚至模拟出的情绪反应。这些因素共同作用，使得智能体在重复互动中逐渐演化出合作、报复、宽恕等复杂的社会行为策略，有些结果甚至与人类在类似社会实验中的表现相似。

**什么是归纳偏置？**

归纳偏置是指机器学习算法在学习过程中所依赖的一系列隐含假设或偏好，这些假设帮助算法从有限的训练数据中推断出对未见数据的合理预测。如果没有归纳偏置，算法将无法在训练样本之外进行任何泛化，因为对于任意新输入，都存在无数种可能的输出与已有数据兼容。例如，当我们看到几个点大致呈直线分布时，人类会自然假设它们遵循线性规律，这种"偏好简单规律"的倾向就是一种归纳偏置。在机器学习中，不同的算法具有不同的归纳偏置：线性回归假设数据关系可以用直线描述，决策树倾向于寻找简单的分层决策规则，而卷积神经网络则内置了对局部空间结构和模式平移不变性的偏好。归纳偏置既不是优点也不是缺点，而是学习过程不可避免的组成部分——它使学习成为可能，但也可能限制算法发现与预设假设不符的复杂规律。在多智能体学习中，研究者有时会刻意引入特定的归纳偏置，例如要求通信协议具有某种结构特性，以促进智能体之间发展出更易理解和解释的语言。

**标题中的Emergent patterns，是复杂系统中的涌现吗？**

是的，"Emergent patterns" 中的 "emergent" 正是复杂系统理论中的"涌现"（emergence）概念。

简单来说，涌现是指：当多个简单个体（比如智能体）按照基本规则相互作用时，整个系统会自发产生出单个个体所不具备的、更高层次的复杂行为模式。这些模式不是被预先设计或编程进去的，而是在互动过程中"自然生长"出来的。

举个直观的例子：单个蚂蚁的行为很简单，只会遵循几条基本规则（如跟随信息素、搬运食物）。但当成千上万只蚂蚁互动时，整个蚁群会自发形成复杂的分工体系、高效的觅食路径和精巧的巢穴结构——这些宏观模式无法从单只蚂蚁的行为中预测，只能从群体互动中涌现出来。

在多智能体强化学习中，涌现模式表现为：

- 智能体没有被明确教导"合作"，但在共享奖励的环境中，它们自发学会了互相配合、避免碰撞、甚至主动帮助队友
- 在指代类游戏中，智能体从零开始发展出具有语义结构的通信协议，能用符号组合表达复杂概念
- 在资源有限的环境中，智能体演化出类似人类的策略：短期自私获取资源，但长期学会克制以维持资源可持续性

这正是多智能体系统迷人又困难的地方：我们只需给智能体设定基本的学习目标和环境规则，复杂的社会行为就可能从中自然浮现。

第4章"**涌现行为模式**"中，几项真正具有里程碑意义的研究工作值得特别关注：

Baker等人2020年在《自然》子刊发表的"捉迷藏"研究是该领域最具影响力的工作之一。研究者在3D物理模拟环境中训练智能体玩捉迷藏游戏，没有预先编程任何工具使用行为。随着训练推进，躲藏者自发学会移动箱子搭建障碍物并用斜坡封住入口，而搜寻者则学会协作推动斜坡翻越障碍。这种复杂行为通过"自课程"机制自然涌现：当一方掌握某种策略后，另一方被迫发展出更高级的应对策略，形成持续升级的策略军备竞赛。这项工作首次在复杂物理环境中清晰展示了高级工具使用行为如何从简单规则中自发产生，为理解智能的起源提供了计算视角。

Lazaridou等人2017年关于指代游戏的研究揭示了语言如何从零开始形成。实验中，一个"说话者"智能体需要通过离散符号向"听者"描述图像中的目标物体。经过训练，智能体发展出具有初步语义结构的通信协议：相似物体被分配相似符号，不同属性（如颜色、形状）被编码为符号的不同组成部分。关键发现是，当研究者中途更换听者智能体时，说话者被迫发展出更具组合性和泛化能力的语言，因为新听者无法理解之前形成的任意约定。这项工作表明，语言的结构性可能源于通信环境中的社会压力，而非预设的归纳偏置。

Leibo等人2017年提出的"序贯社会困境"框架将传统一次性博弈扩展到多步决策场景，使研究者能够观察长期策略的演化。在"收获"游戏中，智能体共享有限的可再生资源，面临短期自私采集与长期可持续利用的权衡。实验发现，当团队奖励相对于个体奖励的比重增加时，智能体更倾向于合作；而当环境引入"不公平厌恶"机制时，合作率显著提升。这项工作将人类社会行为的心理学变量（如公平感、互惠性）引入多智能体系统，架起了计算模型与人类社会行为研究之间的桥梁。

Jaderberg等人2019年在《科学》杂志发表的"群体强化学习"工作展示了多智能体系统在复杂3D环境中的突破。研究者在Quake III Arena多人游戏环境中训练智能体，通过群体训练机制（即同时训练大量智能体变体并定期交换策略），智能体自发演化出团队协作、角色分工和战术配合等复杂行为。这项工作不仅在技术上实现了人类水平的游戏表现，更重要的是展示了多智能体互动如何自然催生社会性行为模式，为理解人类社会行为的计算基础提供了新视角。

Hughes等人2018年关于"不公平厌恶"的研究则揭示了社会规范如何促进合作。在公共资源采集任务中，研究者引入了一种机制：当智能体观察到资源分配极度不均时，会触发负面内在奖励。结果表明，具备这种"不公平厌恶"的智能体更愿意牺牲短期利益以维持资源可持续性，甚至主动惩罚过度采集者。这项工作表明，看似非理性的社会情感（如对不公平的反感）可能在进化上具有适应性价值，能有效解决集体行动困境。

这些研究共同表明：复杂的社会行为、语言和规范无需中央设计，只需设定基本的学习规则和环境约束，就能从多智能体互动中自然产生。这种"自下而上"的生成机制，为理解人类社会行为的起源提供了新的计算视角，也对构建具有社会智能的人工系统具有重要启示。

### 5  Current challenges

多智能体深度强化学习面临几个核心挑战。**非平稳性问题**最为基础：当多个智能体同时学习时，每个智能体的策略都在持续变化，导致从单个智能体视角看，环境动态不断改变。这违反了马尔可夫决策过程的基本假设，使学习目标变成"移动靶子"，传统单智能体算法难以直接应用。

**部分观测**是另一个普遍问题。在真实环境中，智能体通常无法获取环境的完整全局状态，只能观察到局部信息。单次观测往往不足以反映环境全貌及其历史演变，使得环境呈现非马尔可夫特性。这种信息缺失不仅增加决策难度，还会引发"懒惰智能体"现象——在合作任务中，某些智能体可能依赖队友承担主要工作而自身不积极学习。应对方法主要有两类：一是通过通信交换局部观测以弥补信息缺口；二是引入记忆机制，例如使用循环神经网络存储历史交互数据，帮助智能体在信息不完整时做出合理推断。

**信用分配**问题主要出现在合作场景。当多个智能体共享同一个团队奖励信号时，个体难以判断自己具体贡献了多少。例如在足球比赛中进球，是传球者、跑位者还是射门者起了关键作用？缺乏细粒度的贡献评估，使得智能体可能无法准确学习哪些行为真正有价值。研究者通过价值函数分解、边际化技术或逆向强化学习等方法尝试解决这一问题。

**协调挑战**要求智能体在行动选择上达成共识，尤其在合作任务中需要联合动作优化整体目标。当环境存在随机性或观测不完整时，协调变得更加困难，可能出现"行动遮蔽"现象——一个智能体的探索行为干扰其他智能体的学习过程，导致系统陷入次优解。解决途径包括构建其他智能体的行为模型、采用分层抽象方法，或在训练阶段共享额外信息。

**可扩展性限制**了方法的实际应用。随着智能体数量增加，联合状态空间和动作空间呈指数级膨胀，计算复杂度急剧上升。虽然参数共享、课程学习、均值场近似等技术能在一定程度上缓解，但在异构智能体或复杂拓扑结构场景中仍面临瓶颈。此外，训练出的策略还需对其他智能体的行为变化保持鲁棒性，避免过拟合到特定对手策略。

**通信学习**本身也构成独立挑战。智能体需要自主决定何时通信、与谁通信、传递什么信息，并确保发展出的通信协议具有共同语义基础。实践中可能出现隐式通信难以解释、智能体学会欺骗性信号，或即使存在通信通道也选择不使用等问题。研究者通过广播式、目标导向式或网络化通信架构，结合注意力机制和门控策略来提升通信效率与可解释性。

这些挑战相互交织，单一解决方案往往难以全面应对。当前主流实践倾向于采用集中式训练分布式执行框架，在训练阶段共享全局信息以缓解非平稳性和部分观测问题，同时保留实际部署时的分布式特性以保证可扩展性。

### 6  Discussion

作者强调，尽管近年来在复杂环境中的行为涌现方面取得显著进展，但多数成功案例仍局限于高度结构化的仿真环境，与真实世界存在明显差距。一个关键观察是，当前研究**过度依赖环境设计**来引导智能体行为，例如通过精心调整奖励函数或环境规则促使合作产生，而非让智能体真正理解合作的内在价值。这种"环境工程"虽然能产生令人印象深刻的演示效果，却难以迁移到开放、动态的真实场景。

**理论与实践之间的脱节**。多智能体系统理论上存在多种均衡概念，如纳什均衡、帕累托最优等，但实际算法很少以这些理论目标为导向进行设计，更多是依靠经验性技巧（如经验回放缓冲区、目标网络）来稳定训练。这种工程驱动而非理论驱动的发展模式，使得算法行为难以预测和解释，也限制了方法的泛化能力。作者呼吁加强理论基础建设，例如发展适用于非平稳环境的收敛性分析，或建立多智能体学习的样本复杂度理论。

多数研究仅在固定对手或特定环境配置下评估性能，**缺乏对策略鲁棒性的系统检验**。一个在特定对手面前表现优异的智能体，面对策略稍有变化的对手时可能完全失效。更合理的评估应包含对手策略的多样性测试、环境扰动下的稳定性检验，以及跨任务迁移能力的衡量。作者建议建立标准化的评估协议，类似单智能体强化学习中的Atari或MuJoCo基准，以促进公平比较和可复现研究。

最后，讨论部分展望了几个关键发展方向：一是构建更具生态效度的环境，例如包含开放世界、长期记忆需求和多时间尺度决策的场景；二是发展可解释的多智能体系统，使人类能够理解并信任智能体的决策逻辑；三是探索人机混合团队中的学习机制，这既是技术挑战也是社会需求。作者总结道，多智能体深度强化学习的终极目标不应仅是让机器在游戏中击败人类，而是理解智能体如何在复杂社会环境中自主发展出适应性、鲁棒性和可扩展的协作能力，这一目标的实现将需要算法创新、认知科学和伦理思考的深度融合。